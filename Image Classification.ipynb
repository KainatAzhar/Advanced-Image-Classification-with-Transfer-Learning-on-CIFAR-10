{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced Image Classification with Transfer Learning on CIFAR-10\n",
    "\n",
    "### Project Overview\n",
    "This project showcases an advanced approach to image classification by implementing **transfer learning** and **data augmentation**. Instead of building a model from scratch, it leverages a pre-trained VGG16 network to achieve high accuracy on the CIFAR-10 dataset with minimal training time. This project demonstrates proficiency in using modern deep learning techniques to solve computer vision problems efficiently.\n",
    "\n",
    "### Dataset\n",
    "The model was trained and evaluated on the **CIFAR-10 dataset**, which consists of 60,000 32x32 color images across 10 classes:\n",
    "- airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "### Methodology\n",
    "1.  **Transfer Learning:** A pre-trained **VGG16 model**, which was trained on the massive ImageNet dataset, was used as the convolutional base. The layers of the VGG16 model were **frozen** to utilize their learned feature extraction capabilities without modifying their weights.\n",
    "2.  **Custom Classifier:** A new classifier was added on top of the VGG16 base, consisting of a `Flatten` layer, a `Dense` layer with `relu` activation, a `Dropout` layer to prevent overfitting, and a final `Dense` layer for the 10-class classification.\n",
    "3.  **Data Augmentation:** An `ImageDataGenerator` was used to apply real-time transformations (e.g., rotation, shifting, flipping) to the training images. This significantly increases the effective size of the training set, helping the model generalize better to new data.\n",
    "4.  **Training and Evaluation:** The model was compiled with the `Adam` optimizer with a low learning rate (0.0001) to fine-tune the new layers. It was trained for 20 epochs, and its performance was monitored on a separate test set.\n",
    "\n",
    "### Results\n",
    "The model achieved a significantly higher test accuracy (expected to be in the 80-85% range) compared to a basic CNN trained from scratch. This demonstrates the power and efficiency of transfer learning, especially for datasets with limited data. The training and validation loss plots show a stable and consistent improvement.\n",
    "\n",
    "### Technologies Used\n",
    "- Python\n",
    "- TensorFlow / Keras\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Jupyter Notebook"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1r4H1G4B7bO",
    "outputId": "321a4855-4603-490b-d36c-941e737190eb"
   },
   "outputs": [],
   "source": [
    "# Project 1: Advanced Image Classification with Transfer Learning and Data Augmentation\n",
    "\n",
    "# --- Section 1: Setup and Data Loading ---\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Define class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# --- Section 2: Building the Transfer Learning Model ---\n",
    "\n",
    "print(\"Building the Transfer Learning model (VGG16)...\")\n",
    "\n",
    "# Load the VGG16 model, pre-trained on ImageNet.\n",
    "# We set include_top=False to exclude the final classification layers.\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Freeze the base model layers to prevent them from being updated during training.\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the new model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n",
    "\n",
    "# --- Section 3: Data Augmentation ---\n",
    "\n",
    "print(\"Applying data augmentation to the training data...\")\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(train_images)\n",
    "\n",
    "# --- Section 4: Compiling and Training ---\n",
    "\n",
    "print(\"Compiling and training the model...\")\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
    "                    epochs=20,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# --- Section 5: Visualizing and Evaluating Results ---\n",
    "\n",
    "print(\"Visualizing model performance...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluating the final model on the test dataset...\")\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc*100:.2f}%')"
   ]
  }
 ]
}
